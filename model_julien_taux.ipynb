{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        746.0\n",
      "1        549.0\n",
      "2        503.0\n",
      "3        500.0\n",
      "4        525.0\n",
      "         ...  \n",
      "29004    649.0\n",
      "29005    473.0\n",
      "29006    283.0\n",
      "29007    174.0\n",
      "29008    138.0\n",
      "Name: Débit horaire, Length: 29009, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from tslearn.metrics import dtw, dtw_path\n",
    "from tslearn.barycenters import dtw_barycenter_averaging\n",
    "\n",
    "df_train = pd.read_pickle(os.path.join(r\"data/df_train.pkl\"))\n",
    "\n",
    "df_test = pd.read_pickle(os.path.join(r\"data/df_test.pkl\"))\n",
    "\n",
    "df_train = pd.concat([df_train,df_test], ignore_index=True)\n",
    "# print(df_train)\n",
    "# print(df_test)\n",
    "\n",
    "print(df_train[\"Débit horaire\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2]\n",
      "[0.  1.2 1.6 0.8 2.7 3.6 1.8 1.3 0.9 0.1 0.2 0.7 1.  0.5 1.1 1.5 0.4 0.6\n",
      " 0.3 2.1 1.4 1.7 2.  1.9 2.6 3.5 2.2 2.4 3.2 4.4 5.9 3.  2.8 4.2 2.9 2.5\n",
      " 3.7 4.  3.8 2.3]\n"
     ]
    }
   ],
   "source": [
    "mapper = {'AV_Champs_Elysees': 0, 'Convention': 1, 'Sts_Peres': 2}\n",
    "df_train['Libelle'] = df_train['Libelle'].map(mapper)\n",
    "df_test['Libelle'] = df_test['Libelle'].map(mapper)\n",
    "\n",
    "mapper_2 = {'Invalide': \"0\", 'Barré': \"1\"}\n",
    "df_train['Etat arc'] = df_train['Etat arc'].map(mapper_2)\n",
    "df_test['Etat arc'] = df_test['Etat arc'].map(mapper_2)\n",
    "print(df_train[\"Libelle\"].unique())\n",
    "\n",
    "print(df_train[\"precipMM\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Libelle', 'Date et heure de comptage', 'Débit horaire',\n",
      "       'Taux d'occupation', 'Etat trafic', 'Etat arc', 'filename', 'Date',\n",
      "       'Jour de la semaine_0', 'Jour de la semaine_1', 'Jour de la semaine_2',\n",
      "       'Jour de la semaine_3', 'Jour de la semaine_4', 'Jour de la semaine_5',\n",
      "       'Jour de la semaine_6', 'Etat du confinement', 'Couvre-feu',\n",
      "       'Jour férié', 'Vacances scolaires',\n",
      "       'Date des prochaines vacances scolaires',\n",
      "       'Temps avant les prochaines vacances scolaires', 'tempC',\n",
      "       'windspeedKmph', 'winddirDegree', 'weatherCode', 'precipMM', 'humidity',\n",
      "       'visibility', 'pressure', 'cloudcover', 'HeatIndexC', 'DewPointC',\n",
      "       'WindChillC', 'WindGustKmph', 'FeelsLikeC', 'hourly_uvIndex',\n",
      "       'maxtempC', 'mintempC', 'avgtempC', 'totalSnow_cm', 'sunHour',\n",
      "       'daily_uvIndex', 'sunrise', 'sunset', 'moon_phase', 'moon_illumination',\n",
      "       'Journée'],\n",
      "      dtype='object')\n",
      "Libelle                                                    int64\n",
      "Date et heure de comptage                         datetime64[ns]\n",
      "Débit horaire                                            float64\n",
      "Taux d'occupation                                        float64\n",
      "Etat trafic                                                int64\n",
      "Etat arc                                                  object\n",
      "filename                                                  object\n",
      "Date                                                      object\n",
      "Jour de la semaine_0                                       uint8\n",
      "Jour de la semaine_1                                       uint8\n",
      "Jour de la semaine_2                                       uint8\n",
      "Jour de la semaine_3                                       uint8\n",
      "Jour de la semaine_4                                       uint8\n",
      "Jour de la semaine_5                                       uint8\n",
      "Jour de la semaine_6                                       uint8\n",
      "Etat du confinement                                        int64\n",
      "Couvre-feu                                                  bool\n",
      "Jour férié                                                  bool\n",
      "Vacances scolaires                                          bool\n",
      "Date des prochaines vacances scolaires            datetime64[ns]\n",
      "Temps avant les prochaines vacances scolaires    timedelta64[ns]\n",
      "tempC                                                      int64\n",
      "windspeedKmph                                              int64\n",
      "winddirDegree                                              int64\n",
      "weatherCode                                                int64\n",
      "precipMM                                                 float64\n",
      "humidity                                                   int64\n",
      "visibility                                                 int64\n",
      "pressure                                                   int64\n",
      "cloudcover                                                 int64\n",
      "HeatIndexC                                                 int64\n",
      "DewPointC                                                  int64\n",
      "WindChillC                                                 int64\n",
      "WindGustKmph                                               int64\n",
      "FeelsLikeC                                                 int64\n",
      "hourly_uvIndex                                             int64\n",
      "maxtempC                                                   int64\n",
      "mintempC                                                   int64\n",
      "avgtempC                                                   int64\n",
      "totalSnow_cm                                             float64\n",
      "sunHour                                                  float64\n",
      "daily_uvIndex                                              int64\n",
      "sunrise                                           datetime64[ns]\n",
      "sunset                                            datetime64[ns]\n",
      "moon_phase                                                object\n",
      "moon_illumination                                          int64\n",
      "Journée                                                     bool\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df_train.columns)\n",
    "#cat_columns = [\"Etat trafic\", \"Etat arc\", 'Etat du confinement', 'Couvre-feu',\n",
    "#       'Jour férié']\n",
    "cat_columns = [\"Etat trafic\", \"Etat arc\", 'Jour de la semaine_0', 'Jour de la semaine_1', 'Jour de la semaine_2',\n",
    "       'Jour de la semaine_3', 'Jour de la semaine_4', 'Jour de la semaine_5',\n",
    "       'Jour de la semaine_6', 'Etat du confinement', 'Couvre-feu',\n",
    "       'Jour férié', 'Vacances scolaires']\n",
    "bool_columns = ['Couvre-feu',\n",
    "       'Jour férié', 'Vacances scolaires']\n",
    "object_columns = [\"Etat arc\"]\n",
    "# df_meta_train = pd.DataFrame()\n",
    "# #df_meta_train[cat_columns] = df_train[cat_columns].apply(lambda x: x.astype(pd.CategoricalDtype(categories=df_meta.index)).cat.codes)\n",
    "# df_meta_train[cat_columns] = df_train[cat_columns]\n",
    "# df_meta_train['Libelle'] = df_train[\"Libelle\"].astype(str)\n",
    "# df_meta_train.index = df_meta_train['Libelle']\n",
    "\n",
    "# df_meta_test = pd.DataFrame()\n",
    "# #df_meta_train[cat_columns] = df_train[cat_columns].apply(lambda x: x.astype(pd.CategoricalDtype(categories=df_meta.index)).cat.codes)\n",
    "# df_meta_test[cat_columns] = df_test[cat_columns]\n",
    "# df_meta_test['Libelle'] = df_test[\"Libelle\"].astype(str)\n",
    "# df_meta_test.index = df_meta_test['Libelle']\n",
    "\n",
    "# print(df_meta_test.index)\n",
    "# print(df_meta_train['Etat arc'].unique())\n",
    "\n",
    "\n",
    "# cat_code = dict(enumerate(df_meta_train.index))\n",
    "# cat_code_reverse = {v:k for k,v in cat_code.items()}\n",
    "\n",
    "print(df_train.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(l, size, step):\n",
    "    \"\"\" Yield successive n-sized chunks from l.\n",
    "    \"\"\"\n",
    "    l = l[:(len(l) // size) * size]\n",
    "    return [x for x in [l[i : i + size] for i in range(0, len(l), step)] if len(x) == size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        0\n",
      "1        0\n",
      "2        0\n",
      "3        0\n",
      "4        0\n",
      "        ..\n",
      "29004    2\n",
      "29005    2\n",
      "29006    2\n",
      "29007    2\n",
      "29008    2\n",
      "Name: Libelle, Length: 29009, dtype: int64\n",
      "[]\n",
      "0        False\n",
      "1        False\n",
      "2        False\n",
      "3        False\n",
      "4         True\n",
      "         ...  \n",
      "29004    False\n",
      "29005    False\n",
      "29006    False\n",
      "29007    False\n",
      "29008    False\n",
      "Name: Journée, Length: 29009, dtype: bool\n"
     ]
    }
   ],
   "source": [
    "print(df_train[\"Libelle\"])\n",
    "print(df_test[\"Libelle\"].unique())\n",
    "print(df_train['Journée'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paramètres\n",
    "N_input = 14*24\n",
    "N_output = 7*24\n",
    "\n",
    "p_test = 0\n",
    "#meta_specs = {'cat_input_dim': (len(cat_columns), len(df_meta_train.index)),\n",
    "#              'embed_dim': 8}\n",
    "optimizer_parameters = {'lr': 0.001}\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "#visibility \n",
    "metatemp_columns = cat_columns + ['windspeedKmph', 'weatherCode', 'precipMM', 'cloudcover', 'HeatIndexC', 'maxtempC', 'mintempC']\n",
    "num_columns = ['windspeedKmph', 'weatherCode', 'precipMM', 'cloudcover', 'HeatIndexC', 'maxtempC', 'mintempC']\n",
    "df_train[bool_columns] = df_train[bool_columns].apply(lambda x : x.astype(int))\n",
    "#df_test[bool_columns] = df_test[bool_columns].apply(lambda x : x.astype(int))\n",
    "df_train[object_columns] = df_train[object_columns].apply(lambda x : x.astype(int))\n",
    "\n",
    "metatemp_data_train = df_train.sort_values(['Date et heure de comptage', 'Libelle']).groupby('Libelle')[metatemp_columns].apply(lambda x: chunks(x.values, size=N_input+N_output, step=1))\n",
    "#metatemp_data_test = df_test.sort_values(['Date et heure de comptage', 'Libelle']).groupby('Libelle')[metatemp_columns].apply(lambda x: chunks(x.values, size=N_input+N_output, step=1))\n",
    "\n",
    "# print(metatemp_data_train.head())\n",
    "# print(metatemp_data_train[0])\n",
    "# print(len(metatemp_data_train.values[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1.   0.   0.   0.   0.   0.   1.   0.   0.   0.   0.   1.   1.  11.\n",
      " 143.   0.  79.  11.  16.   9.]\n",
      "[0 1 2]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "data_train = df_train.sort_values(['Date et heure de comptage', 'Libelle']).groupby('Libelle')[\"Taux d'occupation\"].apply(lambda x: chunks(x.values, size=N_input+N_output, step=1))\n",
    "\n",
    "#print(data.values)\n",
    "data_numpy_train = np.array([np.array(x) for x in data_train.values])\n",
    "#data_numpy_test = np.array([np.array(x) for x in data_test.values])\n",
    "metatemp_data_numpy_train = np.array([np.array(x) for x in metatemp_data_train.values])\n",
    "#metatemp_data_numpy_test = np.array([np.array(x) for x in metatemp_data_test.values])\n",
    "#data_numpy.astype(float)\n",
    "\n",
    "print(metatemp_data_numpy_train[0][0][0])\n",
    "\n",
    "rues = np.array([x for x in data_train.index])\n",
    "\n",
    "\n",
    "# meta_cat_train = metadata_train[cat_columns].values.astype(int)\n",
    "# meta_cat_train = np.array([np.array(x) for x in meta_cat_train.values])\n",
    "\n",
    "# metadata_test = df_meta_test.loc[rues]\n",
    "# meta_cat_test = metadata_test[cat_columns].values.astype(int)\n",
    "# meta_cat_test = np.array([np.array(x) for x in meta_cat_test.values])\n",
    "print(rues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "(9073, 504, 20)\n",
      "(9073, 504)\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# Mélange les rues\n",
    "print(len(data_numpy_train)==len(metatemp_data_numpy_train))\n",
    "ind_train = np.arange(len(data_numpy_train))\n",
    "np.random.shuffle(ind_train)\n",
    "#ind_test = np.arange(len(data_numpy_test))\n",
    "#np.random.shuffle(ind_test)\n",
    "data_numpy = data_numpy_train[ind_train]\n",
    "#data_numpy_test = data_numpy_test[ind_test]\n",
    "# meta_cat_train = meta_cat_train[ind_train]\n",
    "# meta_cat_test = meta_cat_test[ind_test]\n",
    "metatemp_data_numpy = metatemp_data_numpy_train[ind_train]\n",
    "#metatemp_data_numpy_test = metatemp_data_numpy_test[ind_test]\n",
    "print(metatemp_data_numpy_train[0].shape)\n",
    "print(data_numpy_train[0].shape)\n",
    "\n",
    "metatemp_data_numpy_train = []\n",
    "metatemp_data_numpy_test = []\n",
    "data_numpy_test = []\n",
    "data_numpy_train = []\n",
    "#data_numpy_test = data_numpy[nb_train:]\n",
    "#data_numpy_train = data_numpy[:nb_train]\n",
    "print(len(data_numpy))\n",
    "for i in range(len(metatemp_data_numpy)):\n",
    "    nb_train = int(len(data_numpy[i])*(1-p_test))\n",
    "    metatemp_data_numpy_train.append(metatemp_data_numpy[i][:nb_train])\n",
    "    metatemp_data_numpy_test.append(metatemp_data_numpy[i][nb_train:])\n",
    "    data_numpy_test.append(data_numpy[i][nb_train:])\n",
    "    data_numpy_train.append(data_numpy[i][:nb_train])\n",
    "# print(metatemp_data_numpy_train[0].shape)\n",
    "# print(data_numpy_train[0].shape)\n",
    "# print(metatemp_data_numpy_train[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatène chaque départements ensembles\n",
    "\n",
    "# meta_cat_train = np.concatenate(meta_cat_train)\n",
    "# meta_cat_test = np.concatenate(meta_cat_test)\n",
    "data_numpy_train = np.concatenate(data_numpy_train)\n",
    "data_numpy_test = np.concatenate(data_numpy_test)\n",
    "metatemp_data_numpy_train = np.concatenate(metatemp_data_numpy_train)\n",
    "metatemp_data_numpy_test = np.concatenate(metatemp_data_numpy_test)\n",
    "\n",
    "# data_numpy_train[data_numpy_train < 0] = 0\n",
    "# data_numpy_test[data_numpy_test < 0] = 0\n",
    "# print(metatemp_data_numpy_train.shape)\n",
    "# print(metatemp_data_numpy_train[0][0][13:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.34586197e-01 6.81377771e-02 3.50686497e-01 3.50686497e-01\n",
      " 3.50686497e-01 3.48165997e-01 3.50686497e-01 3.48090209e-01\n",
      " 3.50462225e-01 1.08340305e+00 3.06357718e-01 1.71756402e-01\n",
      " 4.67703726e-01 6.63027985e+00 7.38648801e+01 3.03395594e-01\n",
      " 3.41099223e+01 7.02329963e+00 7.61426246e+00 5.74533035e+00]\n",
      "(0, 504, 20)\n",
      "(27219, 504, 20)\n"
     ]
    }
   ],
   "source": [
    "# Normalise par mean-std calculées sur le data de train\n",
    "std = data_numpy_train.std((0,1))[None]\n",
    "mean = data_numpy_train.mean((0,1))[None]\n",
    "meta_std = metatemp_data_numpy_train[:][:][13:].std((0,1))[None]\n",
    "meta_mean = metatemp_data_numpy_train[:][:][13:].mean((0,1))[None]\n",
    "print(meta_std[None][0][0])\n",
    "\n",
    "\n",
    "data_numpy_train = (data_numpy_train-mean[None])/std[None]\n",
    "data_numpy_test = (data_numpy_test-mean[None])/std[None]\n",
    "for i in range (len(metatemp_data_numpy_train)):\n",
    "    for j in range (len(metatemp_data_numpy_train[i])):\n",
    "        for k in range (len(num_columns)):\n",
    "            metatemp_data_numpy_train[i][j][len(cat_columns)+k] = (metatemp_data_numpy_train[i][j][len(cat_columns)+k]-meta_mean[None][0][0][len(cat_columns)+k])/meta_std[None][0][0][len(cat_columns)+k]\n",
    "\n",
    "for i in range (len(metatemp_data_numpy_test)):\n",
    "    for j in range (len(metatemp_data_numpy_test[i])):\n",
    "        for k in range (len(num_columns)):\n",
    "            metatemp_data_numpy_test[i][j][len(cat_columns)+k] = (metatemp_data_numpy_test[i][j][len(cat_columns)+k]-meta_mean[None][0][0][len(cat_columns)+k])/meta_std[None][0][0][len(cat_columns)+k]\n",
    "# print(metatemp_data_numpy_train[0][0][13:])\n",
    "print(metatemp_data_numpy_test.shape)\n",
    "print(metatemp_data_numpy_train.shape)\n",
    "def rescale(data):\n",
    "    return data * std + mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27219, 336, 1)\n",
      "(27219, 336, 20)\n"
     ]
    }
   ],
   "source": [
    "# Sépare en input/output\n",
    "X_train, y_train = data_numpy_train[:, :N_input], data_numpy_train[:, N_input:]\n",
    "X_test, y_test = data_numpy_test[:, :N_input], data_numpy_test[:, N_input:]\n",
    "#print(metatemp_data_numpy_test)\n",
    "X_meta_train, y_meta_train = metatemp_data_numpy_train[:, :N_input, :], metatemp_data_numpy_train[:, N_input:, :]\n",
    "X_meta_test, y_meta_test = metatemp_data_numpy_test[:, :N_input, :], metatemp_data_numpy_test[:, N_input:, :]\n",
    "X_train = X_train[:,:,None]\n",
    "y_train = y_train[:,:,None]\n",
    "X_test = X_test[:,:,None]\n",
    "y_test = y_test[:,:,None]\n",
    "print(X_train.shape)\n",
    "print(X_meta_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install a pip package in the current Jupyter kernel\n",
    "# import sys\n",
    "# !{sys.executable} -m pip install torch===1.6.0 torchvision===0.7.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "import torch\n",
    "\n",
    "#import torch\n",
    "device = torch.device('cuda:0')\n",
    "#print(type(X_train))\n",
    "#print(data_numpy.dtype)\n",
    "#print(torch.from_numpy(X_train))\n",
    "#print(torch.from_numpy(y_train))\n",
    "#print(torch.from_numpy(y_meta_train))\n",
    "\n",
    "dataset_train = torch.utils.data.TensorDataset(torch.FloatTensor(X_train),\n",
    "                                                torch.FloatTensor(X_meta_train),\n",
    "                                               torch.FloatTensor(y_train),\n",
    "                                              torch.FloatTensor(y_meta_train))\n",
    "dataset_test = torch.utils.data.TensorDataset(torch.FloatTensor(X_test),\n",
    "                                              torch.FloatTensor(X_meta_test),\n",
    "                                              torch.FloatTensor(y_test),\n",
    "                                             torch.FloatTensor(y_meta_test))\n",
    "\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    dataset_train, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    dataset_test, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "for i, data in enumerate(trainloader, 0):\n",
    "    inputs, metatemp_inputs, target, metatemp_target = data\n",
    "#     print(inputs)\n",
    "#     print(metatemp_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fake_baye(outputs, log_sigmas, target):\n",
    "    loss = 1/2 * (log_sigmas + torch.exp(-log_sigmas) * (outputs - target)**2)\n",
    "    return loss.mean()\n",
    "\n",
    "def loss_rescale_factor(log_rescale_factor, outputs, log_sigmas, target):\n",
    "    batch_size = outputs.shape[0]\n",
    "    loss = 1/2 * torch.exp(-2 * log_rescale_factor) * torch.sum(torch.exp(-log_sigmas) * (outputs - target)**2)\n",
    "    loss += batch_size * log_rescale_factor\n",
    "    return loss.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "def train_model(net, loss_type, optimizer_parameters, epochs=50, gamma=0.001,\n",
    "                eval_every=1, Lambda=1, alpha=0.5, beta=0.):\n",
    "    optimizer = torch.optim.Adam(net.parameters(), **optimizer_parameters)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "\n",
    "    if hasattr(net, 'log_rescale_factor'):\n",
    "        optimizer_rescale_factor = torch.optim.Adam([net.log_rescale_factor], **optimizer_parameters)\n",
    "\n",
    "    t = tqdm.notebook.tqdm(range(epochs))\n",
    "    for epoch in t:\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "\n",
    "            net.train()\n",
    "            if hasattr(net, 'log_rescale_factor'):\n",
    "                net.log_rescale_factor.require_grad = False\n",
    "            \n",
    "            \n",
    "            inputs, metatemp_inputs, target, metatemp_target = data\n",
    "            inputs = inputs.to(device)\n",
    "            metatemp_inputs = metatemp_inputs.to(device)\n",
    "            target = target.to(device)\n",
    "            metatemp_target = metatemp_target.to(device)\n",
    "            batch_size, N_output = target.shape[0:2]\n",
    "            \n",
    "            #print(inputs)\n",
    "            #print(metatemp_inputs)\n",
    "            # forward + backward + optimize\n",
    "            \n",
    "            outputs = net(inputs, meta_temporal_input=metatemp_inputs, meta_temporal_target=metatemp_target)\n",
    "            if len(outputs) == 2:\n",
    "                outputs, _ = outputs\n",
    "            else:\n",
    "                outputs, log_sigmas, _ = outputs\n",
    "\n",
    "            loss_mse, loss_shape, loss_temporal = torch.tensor(\n",
    "                0), torch.tensor(0), torch.tensor(0)\n",
    "\n",
    "            loss = 0\n",
    "            if 'mse' in loss_type:\n",
    "                loss_mse = 100 * criterion(outputs, target)\n",
    "                loss += loss_mse\n",
    "\n",
    "            if 'dilate' in loss_type:\n",
    "                loss_dilate, loss_shape, loss_temporal = dilate_loss(\n",
    "                    outputs, target, alpha, gamma, device)\n",
    "                loss += loss_dilate\n",
    "\n",
    "            if 'fake_baye' in loss_type:\n",
    "                loss_fake_baye = fake_baye(outputs, log_sigmas, target)\n",
    "                loss += loss_fake_baye\n",
    "\n",
    "            if 'all_target_regression' in loss_type:\n",
    "                loss_target_week_regression = F.mse_loss(outputs.sum(1), target.sum(1))\n",
    "                loss += loss_target_week_regression\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if hasattr(net, 'log_rescale_factor'):\n",
    "                net.log_rescale_factor.require_grad = True\n",
    "\n",
    "                loss_rescale = loss_rescale_factor(net.log_rescale_factor, outputs.detach(), log_sigmas.detach(),\n",
    "                                                   target)\n",
    "                optimizer_rescale_factor.zero_grad()\n",
    "                loss_rescale.backward()\n",
    "                optimizer_rescale_factor.step()\n",
    "\n",
    "#         if (epoch % eval_every == 0):\n",
    "            \n",
    "#             eval_mse, eval_dtw, eval_tdi = eval_model(net, testloader, gamma)\n",
    "\n",
    "        t.set_postfix(loss=loss.item(),\n",
    "                      loss_shape=loss_shape.item(),\n",
    "                      loss_temporal=loss_temporal.item(),\n",
    "#                       mse=eval_mse,\n",
    "#                       dtw=eval_dtw,\n",
    "#                       tdi=eval_tdi\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(net, loader, gamma):\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    losses_mse = []\n",
    "    losses_dtw = []\n",
    "    losses_tdi = []\n",
    "\n",
    "    net.eval()\n",
    "\n",
    "    for i, data in enumerate(loader, 0):\n",
    "        loss_mse, loss_dtw, loss_tdi = torch.tensor(\n",
    "            0), torch.tensor(0), torch.tensor(0)\n",
    "        # get the inputs\n",
    "        inputs, metatemp_inputs, target, metatemp_target = data\n",
    "        inputs = inputs.to(device)\n",
    "        metatemp_inputs = metatemp_inputs.to(device)\n",
    "        target = target.to(device)\n",
    "        metatemp_target = metatemp_target.to(device)\n",
    "        \n",
    "        batch_size, N_output = target.shape[0:2]\n",
    "        outputs = net(inputs, meta_temporal_input=metatemp_inputs, meta_temporal_target=metatemp_target)\n",
    "        if len(outputs) == 2:\n",
    "            outputs, _ = outputs\n",
    "        else:\n",
    "            outputs, prewarp_outputs, warp_matrix = outputs\n",
    "\n",
    "        # MSE\n",
    "        loss_mse = criterion(target, outputs)\n",
    "        loss_dtw, loss_tdi = 0, 0\n",
    "        # DTW and TDI\n",
    "        for k in range(batch_size):\n",
    "            target_k_cpu = target[k, :, 0:1].view(-1).detach().cpu().numpy()\n",
    "            output_k_cpu = outputs[k, :, 0:1].view(-1).detach().cpu().numpy()\n",
    "\n",
    "            loss_dtw += dtw(target_k_cpu, output_k_cpu)\n",
    "            path, sim = dtw_path(target_k_cpu, output_k_cpu)\n",
    "\n",
    "            Dist = 0\n",
    "            for i, j in path:\n",
    "                Dist += (i - j) * (i - j)\n",
    "            loss_tdi += Dist / (N_output * N_output)\n",
    "\n",
    "        loss_dtw = loss_dtw / batch_size\n",
    "        loss_tdi = loss_tdi / batch_size\n",
    "\n",
    "        # print statistics\n",
    "        losses_mse.append(loss_mse.item())\n",
    "        losses_dtw.append(loss_dtw)\n",
    "        losses_tdi.append(loss_tdi)\n",
    "\n",
    "    return np.array(losses_mse).mean(), np.array(losses_dtw).mean(), np.array(losses_tdi).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, enc_hid_dim, dec_hid_dim, num_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.enc_hid_dim = enc_hid_dim\n",
    "        self.num_layers = num_layers\n",
    "      \n",
    "        self.rnn = nn.GRU(input_dim, enc_hid_dim,\n",
    "                          num_layers=num_layers,\n",
    "                          bidirectional=True,\n",
    "                          batch_first=True,\n",
    "                          dropout=dropout)\n",
    "\n",
    "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src, meta_temporal, hidden=None):\n",
    "       \n",
    "        if meta_temporal is not None:\n",
    "            src = torch.cat([src, meta_temporal], dim=2)\n",
    "        \n",
    "        \n",
    "\n",
    "        outputs, hidden = self.rnn(src, hidden)\n",
    "\n",
    "        # outputs = [batch size, src len, hid dim * num directions]\n",
    "        # hidden = [n layers * num directions, batch size, hid dim]\n",
    "\n",
    "        hidden = torch.cat((hidden[::2, :, :], hidden[1::2, :, :]), dim=2)\n",
    "        hidden = self.dropout(hidden)\n",
    "        hidden = torch.tanh(self.fc(hidden))\n",
    "\n",
    "        # outputs = [batch size, src len, enc hid dim * 2]\n",
    "        # hidden = [batch size, dec hid dim]\n",
    "\n",
    "        return outputs, hidden\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n",
    "        self.v = nn.Linear(dec_hid_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "\n",
    "        # hidden = [batch size, dec hid dim]\n",
    "        # encoder_outputs = [batch size, src len, enc hid dim * 2]\n",
    "\n",
    "        src_len = encoder_outputs.shape[1]\n",
    "\n",
    "        # repeat decoder hidden state src_len times\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "\n",
    "        # hidden = [batch size, src len, dec hid dim]\n",
    "        # encoder_outputs = [batch size, src len, enc hid dim * 2]\n",
    "\n",
    "        energy = torch.tanh(\n",
    "            self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
    "\n",
    "        # energy = [batch size, src len, dec hid dim]\n",
    "\n",
    "        attention = self.v(energy).squeeze(2)\n",
    "\n",
    "        # attention= [batch size, src len]\n",
    "\n",
    "        return F.softmax(attention, dim=1)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, enc_hid_dim, dec_hid_dim, fc_dim,\n",
    "                 num_layers, dropout, attention, meta_temporal_dim, bayesian=False, embed_dim=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.output_dim = output_dim\n",
    "        self.attention = attention\n",
    "        self.num_layers = num_layers\n",
    "        self.enc_hid_dim = enc_hid_dim\n",
    "        self.dec_hid_dim = dec_hid_dim\n",
    "        self.fc_dim = fc_dim\n",
    "        \n",
    "        self.embed_dim = embed_dim if embed_dim is not None else 0\n",
    "        self.meta_temporal_dim = meta_temporal_dim if meta_temporal_dim is not None else 0\n",
    "        \n",
    "        self.bayesian = bayesian\n",
    "        \n",
    "        if self.bayesian:\n",
    "            self.out_log_sigma = nn.Linear(fc_dim + output_dim, output_dim)\n",
    "\n",
    "            self.rnn = nn.GRU((enc_hid_dim * 2) + (input_dim * 2) + meta_temporal_dim,\n",
    "                              dec_hid_dim,\n",
    "                              num_layers=num_layers,\n",
    "                              batch_first=True,\n",
    "                              dropout=dropout)\n",
    "            self.fc_1 = nn.Linear(\n",
    "                (enc_hid_dim * 2) + dec_hid_dim + (input_dim * 2) + self.embed_dim, fc_dim)\n",
    "\n",
    "        else:\n",
    "            self.rnn = nn.GRU((enc_hid_dim * 2) + output_dim + meta_temporal_dim,\n",
    "                              dec_hid_dim,\n",
    "                              num_layers=num_layers,\n",
    "                              batch_first=True,\n",
    "                              dropout=dropout)\n",
    "\n",
    "            self.fc_1 = nn.Linear(\n",
    "                (enc_hid_dim * 2) + dec_hid_dim + output_dim + self.embed_dim, fc_dim)\n",
    "        \n",
    "        self.fc_2 = nn.Linear(fc_dim, fc_dim)\n",
    "        self.out = nn.Linear(fc_dim, output_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, inputs, hidden, encoder_outputs, meta_temporal, embeddings=None):\n",
    "\n",
    "        a = self.attention(hidden[-1, :, :], encoder_outputs)\n",
    "\n",
    "        # a = [batch size, src len]\n",
    "\n",
    "        a = a.unsqueeze(1)\n",
    "\n",
    "        # a = [batch size, 1, src len]\n",
    "\n",
    "        weighted = torch.bmm(a, encoder_outputs)\n",
    "\n",
    "        # weighted = [batch size, 1, enc hid dim * 2]\n",
    "\n",
    "        if meta_temporal is not None:\n",
    "            rnn_input = torch.cat((inputs, meta_temporal, weighted), dim=2)\n",
    "        else:\n",
    "            rnn_input = torch.cat((inputs, weighted), dim=2)\n",
    "\n",
    "        # rnn_input = [batch size, 1, (enc hid dim * 2) + emb dim]\n",
    "\n",
    "        output, hidden = self.rnn(rnn_input, hidden)\n",
    "\n",
    "        # output = [batch size, seq len, dec hid dim * n directions]\n",
    "        # hidden = [n layers * n directions, batch size, dec hid dim]\n",
    "\n",
    "        hidden_prediction = torch.cat((output, weighted, inputs), dim=2)\n",
    "        \n",
    "        if embeddings is not None:\n",
    "            hidden_prediction = torch.cat((hidden_prediction, embeddings), dim=2)\n",
    "        \n",
    "        #hidden_prediction = self.fc(hidden_prediction)\n",
    "        hidden_prediction = self.fc_1(F.relu(hidden_prediction))\n",
    "        hidden_prediction = self.fc_2(F.relu(hidden_prediction))\n",
    "\n",
    "        prediction = self.out(F.relu(hidden_prediction))\n",
    "\n",
    "        # prediction = [batch size, output dim]\n",
    "\n",
    "        if not self.bayesian:\n",
    "            return prediction, hidden\n",
    "        else:\n",
    "            log_sigma = self.out_log_sigma(torch.cat([hidden_prediction, prediction], dim=-1))\n",
    "            return prediction, log_sigma, hidden\n",
    "\n",
    "\n",
    "class MetaEmbedder(nn.Module):\n",
    "    def __init__(self, cat_input_dim, num_input_dim, embed_dim):\n",
    "        \"\"\"cat_input_dim should be a tuple (nb_of_inputs, nb_of_categories)\"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.cat_input_dim = cat_input_dim\n",
    "        self.num_input_dim = num_input_dim\n",
    "        self.embed_dim = embed_dim\n",
    "        # self.cat_module = nn.Embedding(cat_input_dim[1], embed_dim)\n",
    "        self.num_module = nn.Linear(num_input_dim, embed_dim)\n",
    "        #self.fc = nn.Linear(self.embed_dim * (cat_input_dim[0] + 1), self.enc_hid_dim * self.num_layers * 2)\n",
    "        self.fc = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "\n",
    "    def forward(self, inputs_cat, inputs_num):\n",
    "        batch_size = inputs_cat.shape[0]\n",
    "        #x_cat = self.cat_module(inputs_cat).reshape(batch_size, -1)\n",
    "        x_num = F.relu(self.num_module(inputs_num))\n",
    "        #x = torch.cat([x_cat, x_num], dim=1)\n",
    "        x = x_num\n",
    "        batch_size = x.shape[0]\n",
    "        out = self.fc(x)\n",
    "        return out\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device, target_size, embedder=None, mc_dropout=False,\n",
    "                use_meta_temporal=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "#         print(device)\n",
    "#         if not torch.cuda.is_available() and self.device is None: \n",
    "#             self.device = -1\n",
    "        \n",
    "        self.target_size = target_size\n",
    "      \n",
    "        self.bayesian = self.decoder.bayesian\n",
    "        self.mc_dropout = mc_dropout\n",
    "        self.use_meta_temporal = use_meta_temporal\n",
    "        \n",
    "        if embedder is not None:\n",
    "            self.embedder = embedder\n",
    "        \n",
    "        if self.bayesian:\n",
    "            #Add a learnable rescale factor as in https://openreview.net/pdf?id=CecZ_0t79q\n",
    "            self.log_rescale_factor = nn.Parameter(torch.ones(1, requires_grad=True))\n",
    "\n",
    "    def forward(self, src, meta_temporal_input, meta_temporal_target, meta_cat=None, meta_num=None,\n",
    "               ):\n",
    "        \n",
    "        batch_size = src.shape[0]\n",
    "\n",
    "        if not self.use_meta_temporal:\n",
    "            meta_temporal_input = None\n",
    "            meta_temporal_target = None\n",
    "        \n",
    "        #print(src)\n",
    "        #print(meta_temporal_input)\n",
    "        encoder_outputs, hidden = self.encoder(src, meta_temporal=meta_temporal_input)\n",
    "        \n",
    "        embeddings = None\n",
    "        if hasattr(self, 'embedder') and meta_cat is not None and meta_num is not None:\n",
    "            embeddings = self.embedder(meta_cat, meta_num).unsqueeze(1)\n",
    "\n",
    "        # tensor to store decoder outputs\n",
    "        outputs = torch.zeros(batch_size,\n",
    "                              self.target_size,\n",
    "                              self.decoder.output_dim).to(self.device)\n",
    "        hiddens = torch.zeros(batch_size,\n",
    "                              self.target_size,\n",
    "                              self.decoder.dec_hid_dim).to(self.device)\n",
    "        \n",
    "        if self.bayesian:\n",
    "            log_sigmas = torch.zeros(batch_size,\n",
    "                                     self.target_size,\n",
    "                                     self.decoder.output_dim).to(self.device)\n",
    "\n",
    "        inputs = src[:, -1, :].unsqueeze(1)\n",
    "        \n",
    "        if self.bayesian:\n",
    "            inputs = torch.cat([inputs, torch.zeros(*inputs.shape).to(inputs.device)], 2)\n",
    "\n",
    "        for t in range(0, self.target_size):\n",
    "            meta_temporal = meta_temporal_target[:, t:t + 1, :] if meta_temporal_target is not None else None\n",
    "            dec_outputs = self.decoder(inputs, hidden, encoder_outputs, embeddings=embeddings,\n",
    "                                      meta_temporal=meta_temporal)\n",
    "\n",
    "            if not self.bayesian:\n",
    "                output, hidden = dec_outputs\n",
    "                outputs[:, t:t + 1, :] = output\n",
    "                hiddens[:, t:t + 1, :] = hidden[-2:-1, :, :].transpose(0, 1)\n",
    "\n",
    "                inputs = output\n",
    "            else:\n",
    "                output, log_sigma, hidden = dec_outputs\n",
    "                outputs[:, t:t + 1, :] = output\n",
    "                hiddens[:, t:t + 1, :] = hidden[-2:-1, :, :].transpose(0, 1)\n",
    "                log_sigmas[:, t:t+1, :] = log_sigma\n",
    "\n",
    "                inputs = torch.cat([output, log_sigma], dim=2)\n",
    "        \n",
    "        #if hasattr(self, 'rescale_factor'):\n",
    "        #    log_sigmas += 2 * self.rescale_factor\n",
    "\n",
    "        if not self.bayesian:\n",
    "            return outputs, hiddens\n",
    "        else:\n",
    "            return outputs, log_sigmas, hiddens\n",
    "\n",
    "def activate_mc_dropout(model):\n",
    "    for m in model.modules():\n",
    "        if (type(m) == nn.Dropout) or (type(m) == nn.GRU):\n",
    "            m.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_net(dropout=0.5, bayesian=False, mc_dropout=False, meta_temporal=True):\n",
    "    embedder = None\n",
    "    \n",
    "    if meta_temporal:\n",
    "        encoder_input_dim = 1 + len(metatemp_columns)\n",
    "        meta_temporal_dim = len(metatemp_columns)\n",
    "    else:\n",
    "        encoder_input_dim = 1\n",
    "        meta_temporal_dim = 0\n",
    "        \n",
    "    encoder = Encoder(input_dim= encoder_input_dim, enc_hid_dim=32, dec_hid_dim=32,\n",
    "                      num_layers=4, dropout=dropout\n",
    "                     ).to(device)\n",
    "    attention = Attention(enc_hid_dim=32, dec_hid_dim=32)\n",
    "    decoder = Decoder(input_dim=1, output_dim= 1, enc_hid_dim=32, dec_hid_dim=32, fc_dim=32,\n",
    "                      num_layers=4, dropout=dropout, attention=attention, bayesian=bayesian,\n",
    "                      embed_dim = None, meta_temporal_dim=meta_temporal_dim\n",
    "                     ).to(device)\n",
    "    net = Seq2Seq(encoder, decoder, device, N_output, mc_dropout=mc_dropout, embedder=embedder, use_meta_temporal=meta_temporal).to(device)\n",
    "\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import wandb\n",
    "print(torch.cuda.device_count())  \n",
    "print(torch.cuda.is_available())   \n",
    "print(torch.version.cuda) \n",
    "nets_lists = {}\n",
    "\n",
    "#wandb.init()\n",
    "#nets_lists['mse_ensembling'] = []\n",
    "nets_lists['meta_mc_dropout'] = []\n",
    "for i in range(4):\n",
    "    net = gen_net(dropout=0.5, bayesian=True, mc_dropout=True, meta_temporal=True)\n",
    "    train_model(net, loss_type=['fake_baye'], optimizer_parameters=optimizer_parameters,\n",
    "            epochs=5, eval_every=10)\n",
    "    nets_lists['meta_mc_dropout'].extend([net]*50)\n",
    "\n",
    "[x.log_rescale_factor for x in nets_lists['meta_mc_dropout']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,x in enumerate(nets_lists['meta_mc_dropout']):\n",
    "    torch.save(x.state_dict(), os.path.join(r\"data/model_final_{}.pth\").format(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nets_lists = {}\n",
    "nets_lists['meta_mc_dropout'] = []\n",
    "\n",
    "#torch.save(x.state_dict(), os.path.join(r\"data/model_2_{}.pth\").format(i))\n",
    "#x.eval()\n",
    "net = gen_net(dropout=0.5, bayesian=True, mc_dropout=True, meta_temporal=True)\n",
    "net.load_state_dict(torch.load(os.path.join(r\"data/modeles_25h/modeles_25h/model_simple_{}.pth\").format(0))) \n",
    "net.eval()\n",
    "nets_lists['meta_mc_dropout'].extend([net]*50)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variational_averaging(preds):\n",
    "    sigma_shape = np.std(preds, axis=0)\n",
    "    pred = np.mean(preds, axis=0)\n",
    "    return pred, sigma_shape, None\n",
    "\n",
    "\n",
    "def variational_averaging_dba(preds):\n",
    "    pred = dtw_barycenter_averaging(preds)\n",
    "\n",
    "    dtw_warps = []\n",
    "    dtw_paths = []\n",
    "    for i in range(len(preds)):\n",
    "        path, dist_i = dtw_path(pred, preds[i])\n",
    "        w_k = np.zeros((len(pred), len(preds[i])))\n",
    "        for i, j in path:\n",
    "            w_k[i, j] = 1.\n",
    "        dtw_paths.append(path)\n",
    "        dtw_warps.append(w_k)\n",
    "\n",
    "    sigma_shape = np.zeros((len(pred), pred.shape[-1]))\n",
    "    for t in range(len(pred)):\n",
    "        values = []\n",
    "        for i in range(len(preds)):\n",
    "            for j, k in dtw_paths[i]:\n",
    "                if j == t:\n",
    "                    values.append(preds[i][k])\n",
    "        sigma_shape[t] = np.std(values, axis=0)\n",
    "\n",
    "    dtw_warps = np.array(dtw_warps)\n",
    "    sigma_time = np.mean(np.abs(np.eye(len(pred)) - dtw_warps))\n",
    "    sigma_time = np.expand_dims(np.expand_dims(sigma_time, 0), -1)\n",
    "\n",
    "    return pred, sigma_shape, sigma_time\n",
    "\n",
    "\n",
    "averaging_fun = {'classical': variational_averaging, 'dba': variational_averaging_dba}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "test_inputs = torch.FloatTensor(X_test).to(device)\n",
    "test_targets = torch.FloatTensor(y_test).to(device)\n",
    "test_metatemp_inputs = torch.FloatTensor(X_meta_test).to(device)\n",
    "test_metatemp_targets = torch.FloatTensor(y_meta_test).to(device)\n",
    "\n",
    "sigmas = {n: {k: np.zeros((*test_targets.shape,)) for k in averaging_fun} for n in nets_lists}\n",
    "\n",
    "predictions = {n: {k: np.zeros((*test_targets.shape,)) for k in averaging_fun} for n in nets_lists}\n",
    "\n",
    "temp_preds = {n: np.zeros((test_targets.shape[0], len(nets_lists[n]), *test_targets.shape[1:],)) for n in nets_lists}\n",
    "temp_sigmas = {n: np.zeros((test_targets.shape[0], len(nets_lists[n]), *test_targets.shape[1:],)) for n in nets_lists}\n",
    "\n",
    "\n",
    "\n",
    "for net_name, nets_list in nets_lists.items():\n",
    "    with torch.no_grad():\n",
    "        for net in nets_list:\n",
    "            net.eval()\n",
    "            if net.mc_dropout:\n",
    "                activate_mc_dropout(net)\n",
    "\n",
    "        for b in range(test_targets.shape[0] // batch_size + int(test_targets.shape[0] % batch_size != 0)):\n",
    "            preds = []\n",
    "            aleatoric_sigmas = []\n",
    "            for net in nets_list:\n",
    "                pred = net(\n",
    "                    test_inputs[batch_size * b:batch_size * (b+1), ...],\n",
    "                    meta_temporal_input=test_metatemp_inputs[batch_size * b:batch_size * (b+1), ...],\n",
    "                    meta_temporal_target=test_metatemp_targets[batch_size * b:batch_size * (b+1), ...],\n",
    "                )\n",
    "                if len(pred) == 2:\n",
    "                    pred, _ = pred\n",
    "                else:\n",
    "                    pred, log_sigma, _ = pred\n",
    "                    sigma = np.exp(log_sigma.cpu().numpy())\n",
    "                    sigma = np.squeeze(rescale(sigma))\n",
    "                    aleatoric_sigmas.append(sigma)\n",
    "\n",
    "                pred = pred.cpu().numpy()\n",
    "                pred = rescale(pred)\n",
    "                preds.append(pred)\n",
    "            aleatoric_sigmas=np.array(aleatoric_sigmas)[:,:,:,None]\n",
    "            temp_preds[net_name][batch_size * b:batch_size * (b+1)] = np.array(preds).transpose((1,0,2,3))\n",
    "            temp_sigmas[net_name][batch_size * b:batch_size * (b+1)] = aleatoric_sigmas.transpose((1,0,2,3))\n",
    "            \n",
    "            \n",
    "            \n",
    "for ind in tqdm.tqdm_notebook(range(test_targets.shape[0])):\n",
    "    #print(ind)\n",
    "    #f = plt.figure(figsize=(24, 12))\n",
    "    k = 1\n",
    "    j = 1\n",
    "    for net_name, nets_list in nets_lists.items():\n",
    "        #preds = []\n",
    "        #aleatoric_sigmas = []\n",
    "#         with torch.no_grad():\n",
    "#             for net in nets_list:\n",
    "#                 net.eval()\n",
    "#                 if net.mc_dropout:\n",
    "#                     activate_mc_dropout(net)\n",
    "#                 pred = net(\n",
    "#                     test_inputs[ind:ind + 1, ...])\n",
    "#                 if len(pred) == 2:\n",
    "#                     pred, _ = pred\n",
    "#                 else:\n",
    "#                     pred, log_sigma, _ = pred\n",
    "#                     sigma = np.exp(log_sigma.cpu().numpy()[0, :, :])\n",
    "#                     sigma = np.squeeze(rescale(sigma))\n",
    "#                     aleatoric_sigmas.append(sigma)\n",
    "\n",
    "#                 pred = pred.cpu().numpy()[0, :, :]\n",
    "#                 pred = rescale(pred)\n",
    "#                 preds.append(pred)\n",
    "\n",
    "        #preds = np.array(preds)\n",
    "        #aleatoric_sigmas = np.array(aleatoric_sigmas)\n",
    "        preds = temp_preds[net_name][ind]\n",
    "        aleatoric_sigmas = temp_sigmas[net_name][ind]\n",
    "\n",
    "        aleatoric_sigmas = np.mean(aleatoric_sigmas, 0) if len(aleatoric_sigmas) != 0 else 0\n",
    "        if hasattr(nets_list[0], 'log_rescale_factor'):\n",
    "            aleatoric_sigmas *= np.mean([np.exp(net.log_rescale_factor.cpu().detach().numpy())\n",
    "                                         for net in nets_list], 0)\n",
    "        \n",
    "        if ind % 10 == 0:\n",
    "            \n",
    "            f = plt.figure(figsize=(24, 12))\n",
    "            \n",
    "        for fun_name, fun in averaging_fun.items():\n",
    "\n",
    "            pred, sigma_shape, sigma_time = fun(preds)\n",
    "            \n",
    "            predictions[net_name][fun_name][ind] = pred\n",
    "\n",
    "            input = test_inputs.detach().cpu().numpy()[ind, :, :]\n",
    "            input = rescale(input)\n",
    "            target = test_targets.detach().cpu().numpy()[ind, :, :]\n",
    "            target = rescale(target)\n",
    "            \n",
    "            epistemic_sigma = 0\n",
    "            if sigma_shape is not None:\n",
    "                epistemic_sigma += sigma_shape**2\n",
    "            if sigma_time is not None:\n",
    "                epistemic_sigma += sigma_time**2\n",
    "            epistemic_sigma = np.sqrt(epistemic_sigma)\n",
    "            \n",
    "            if hasattr(nets_list[0], 'log_rescale_factor'):\n",
    "                epistemic_sigma *= np.mean([np.exp(net.log_rescale_factor.cpu().detach().numpy())\n",
    "                                         for net in nets_list], 0)\n",
    "\n",
    "            sigma = epistemic_sigma\n",
    "            sigmas[net_name][fun_name][ind] = sigma\n",
    "            \n",
    "            aleatoric_sigmas = np.sqrt(aleatoric_sigmas)\n",
    "\n",
    "            #plt.subplot(len(nets_lists) + 1, len(averaging_fun) + 1, j + k)\n",
    "            if ind % 10 == 0:\n",
    "                plt.subplot(len(nets_lists) + 1, len(averaging_fun) + 1, j + k)\n",
    "                \n",
    "                for d in [0]: #range(pred.shape[-1]):\n",
    "                    plt.plot(range(0, N_input), input[:, d], 'b', label='input', linewidth=1)\n",
    "                    plt.plot(range(N_input - 1, N_input + N_output),\n",
    "                             np.concatenate([input[N_input - 1:N_input], target])[:, d],\n",
    "                             'b--', label='target', linewidth=1)\n",
    "                    plt.plot(range(N_input - 1, N_input + N_output),\n",
    "                             np.concatenate([input[N_input - 1:N_input], pred])[:, d],\n",
    "                             'r', label='prediction', linewidth=1)\n",
    "\n",
    "                    plt.fill_between(\n",
    "                        range(N_input - 1, N_input + N_output),\n",
    "                        np.concatenate(\n",
    "                            [input[N_input - 1:N_input], pred + 2 * epistemic_sigma]\n",
    "                        )[:, d],\n",
    "                        np.concatenate(\n",
    "                            [input[N_input - 1:N_input], pred - 2 * epistemic_sigma]\n",
    "                        )[:, d],\n",
    "                        color='orange', alpha=0.3, label='epistemic', linewidth=1)\n",
    "            \n",
    "            if False:\n",
    "                for d in range(pred.shape[-1]):\n",
    "                    plt.fill_between(\n",
    "                        range(N_input - 1, N_input + N_output),\n",
    "                        np.concatenate(\n",
    "                            [input[N_input - 1:N_input], pred + 2 * aleatoric_sigmas]\n",
    "                        )[:, d],\n",
    "                        np.concatenate(\n",
    "                            [input[N_input - 1:N_input], pred - 2 * aleatoric_sigmas]\n",
    "                        )[:, d],\n",
    "                        color='green', alpha=0.15, label='aleatoric', linewidth=1)\n",
    "            #plt.xticks(range(0, 28, 2))\n",
    "            \n",
    "            handles, labels = plt.gca().get_legend_handles_labels()\n",
    "            by_label = dict(zip(labels, handles))\n",
    "            plt.legend(by_label.values(), by_label.keys(),\n",
    "                       loc='upper center', bbox_to_anchor=(0.5, 1.05), ncol=2)\n",
    "\n",
    "            plt.title('{} {}'.format(net_name, fun_name))\n",
    "            j = j + 1\n",
    "        k = k + 1\n",
    "\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stat_check(value, samples, method='max'):\n",
    "    if method == 'max':\n",
    "        return value > (samples.mean() + 3 * samples.std())\n",
    "    else:\n",
    "        return value < (samples.mean() - 3 * samples.std())\n",
    "\n",
    "def highlight(method='max'):\n",
    "    def fun(data, bcolor='rgba(60,179,113,0.5)', tcolor='red'):\n",
    "        '''\n",
    "        highlight the maximum in a Series or DataFrame\n",
    "        '''\n",
    "        m = method if type(method)==str else method[data.name[1]]\n",
    "        if m == 'min':\n",
    "            data = -data\n",
    "        attr = 'background-color: {}'.format(bcolor)\n",
    "\n",
    "        is_max = data == data.max()\n",
    "\n",
    "        diff_significative = stat_check(data[is_max].iloc[0], data[~is_max], method=m)\n",
    "        if diff_significative:\n",
    "            attr += \"; color: {}\".format(tcolor)\n",
    "\n",
    "        return [attr if v else '' for v in is_max]\n",
    "    return fun\n",
    "\n",
    "def rower(data):\n",
    "    s = np.arange(len(data.index)) % 2 != 0\n",
    "    s = pd.concat([pd.Series(s)] * len(data.columns), axis=1) #6 or the n of cols u have\n",
    "    z = pd.DataFrame(np.where(s, '', 'background-color:#f2f2f2'),\n",
    "                 index=data.index, columns=data.columns)\n",
    "    return z\n",
    "\n",
    "def set_style(df):\n",
    "    style = (df.style.apply(rower, axis=None)\n",
    "             #.apply(highlight({'corr_indiv': 'max','corr_sum':'max','mae':'min','rmse':'min','smape':'min'}))\n",
    "             .set_properties(**{'text-align': 'right', 'border': '1px solid black'})\n",
    "             .set_table_styles([{'selector': 'th', 'props': {'text-align': 'right', 'border': '2px solid black'}.items()}])\n",
    "            )\n",
    "    return style\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "\n",
    "def filter_nans(a, b):\n",
    "    cond = np.isnan(a).any(1) | (a == np.inf).any(1) | (a == -np.inf).any(1)\n",
    "    return a[~cond], b[~cond]\n",
    "\n",
    "def smape(pred, target):\n",
    "    pred, target = filter_nans(pred, target)\n",
    "    smape = 100 * np.mean(np.abs(target - pred) / (np.abs(pred) + np.abs(target)), (0,1))\n",
    "    if smape > 100:\n",
    "        smape = 100.\n",
    "    return smape\n",
    "\n",
    "def mae(pred, target):\n",
    "    pred, target = filter_nans(pred, target)\n",
    "    return metrics.mean_absolute_error(pred, target)\n",
    "\n",
    "\n",
    "def rmse(pred, target):\n",
    "    pred, target = filter_nans(pred, target)\n",
    "    return np.mean(np.sqrt(np.sum((pred - target)**2, 1)), 0)\n",
    "\n",
    "def corr_indiv(pred, target):\n",
    "    pred, target = filter_nans(pred, target)\n",
    "    return np.corrcoef(pred.reshape(-1), target.reshape(-1))[0, 1]\n",
    "\n",
    "def corr_sum(pred, target):\n",
    "    pred, target = filter_nans(pred, target)\n",
    "    return np.corrcoef(pred.sum(1), target.sum(1))[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_metrics = {'smape': smape, 'mae': mae, 'rmse': rmse, 'corr_indiv': corr_indiv, 'corr_sum': corr_sum}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_results(predictions, models, averaging_fun=['classical']):\n",
    "    resultats =  {l: {n: {} for n in averaging_fun} for l in list_metrics}\n",
    "    for i, net_name in enumerate(models):\n",
    "        for fun_name in predictions[net_name]:\n",
    "            print(predictions[net_name][fun_name].shape)\n",
    "            print(rescale(y_test)[-1,-15:-1,0])\n",
    "            for metric_name, fun in list_metrics.items(): \n",
    "                m = fun(predictions[net_name][fun_name][:,:,0],rescale(y_test)[:,:,0])\n",
    "                resultats[metric_name][fun_name][net_name] = m\n",
    "                \n",
    "    resultats = pd.DataFrame.from_dict({(j,k): resultats[j][k]\n",
    "                                        for j in resultats.keys()\n",
    "                                        for k in resultats[j].keys()},\n",
    "                                       orient='columns')\n",
    "    print(resultats)\n",
    "    resultats = resultats.stack()\n",
    "    print(resultats)\n",
    "    resultats.rename_axis(['Méthode', 'Aggrégation'], inplace=True)\n",
    "    resultats.rename_axis(['Metric'], axis=1, inplace=True)\n",
    "    return resultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultats_dl = compute_results(predictions, nets_lists, averaging_fun=averaging_fun)\n",
    "set_style(resultats_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(resultats_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
